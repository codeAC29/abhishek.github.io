<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang=""> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <!--<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">-->
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>LinkNet - Network for Semantic Segmentation</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="apple-touch-icon" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" href="favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="favicon-16x16.png" sizes="16x16" />
    <link rel="stylesheet" href="css/normalize.min.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/jquery.fancybox.css">
    <link rel="stylesheet" href="css/flexslider.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/queries.css">
    <link rel="stylesheet" href="css/etline-font.css">
    <link rel="stylesheet" href="bower_components/animate.css/animate.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <script src="js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
</head>
<body id="top">
    <section class="hero">
        <section class="navigation">
            <header>
                <div class="header-content">
                    <div class="logo"><a href="#"><img src="img/logo.png" alt="Sedna logo"></a></div>
                    <div class="header-nav">
                        <nav>
                            <ul class="primary-nav">
                                <li><a href="#demonstration">Demonstraion</a></li>
                                <li><a href="#architecture">Architecture</a></li>
                                <li><a href="#contact">Contact Us</a></li>
                            </ul>
                        </nav>
                    </div>
                    <div class="navicon">
                        <a class="nav-toggle" href="#"><span></span></a>
                    </div>
                </div>
            </header>
        </section>
        <div class="container">
            <div class="row">
                <div class="col-md-10 col-md-offset-1">
                    <div class="hero-content text-center">
                        <h1>LinkNet</h1>
                        <p class="intro">Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation</p>
                        <h3>Abhishek Chaurasia and Eugenio Culurciello</h3>
                        <a href="#" class="btn btn-fill btn-large btn-margin-right">Github</a> <a href="#" class="btn btn-accent btn-large">Paper</a>
                    </div>
                </div>
            </div>
        </div>
        <div class="down-arrow floating-arrow"><a href="#demonstration"><i class="fa fa-angle-down"></i></a></div>
    </section>
    <section class="intro section-padding-small">
        <div class="container">
            <div class="row">
                <p align="justify">
                LinkNet is a <i>light</i> deep neural network architecture designed for performing semantic segmentation, which can be used for tasks such as self-driving vehicles, augmented reality, etc.
                It is capable of giving real-time performance on both GPUs and embedded device such as NVIDIA TX1.
                This network was designed by members of <a href="https://e-lab.github.io">e-Lab</a> at <a href="http://www.purdue.edu">Purdue University</a>.
                Its architecture has been explained in the following blocks.
                LinkNet can process an input image of resolution 1280x720 on TX1 and Titan X at a rate of 2 fps and 19 fps respectively.
                Training and testing scripts can be accessed using the Github <a href="">link</a>.
                </p>
            </div>
        </div>
    </section>
    <section class="blog-intro section-padding-small" id="demonstration">
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-sm-12 col-xs-12 leftcol">
                    <h5>VIDEO DEMONSTRATION ON CITYSCAPES DATASET [1]</h5>
                    <iframe width="550" height="309" src="https://www.youtube.com/embed/1QIWtO2ync8" frameborder="0" allowfullscreen></iframe>
                </div>
                <div class="col-md-6 col-sm-12 col-xs-12 rightcol">
                    <h5>VIDEO DEMONSTRATION ON CAMVID DATASET [2]</h5>
                    <iframe width="550" height="309" src="https://www.youtube.com/embed/M8B3jhvdV3k" frameborder="0" allowfullscreen></iframe>
                </div>
            </div>
        </div>
    </section>
    <section class="features-extra section-padding" id="architecture">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div class="feature-list" align="justify">
                        <h3>Network Architecture</h3>
                        <p>
                        Our proposed deep neural network architecture tries to efficiently share the information learnt by the encoder with the decoder after each downsampling block.
                        This proves to be better than using pooling indices in decoder or just using fully convolutional networks in decoder.
                        Not only this feature forwarding technique gives us good accuracy values, but also enables us to have few parameters in our decoder.
                        </p>
                        <p>
                        The initial block contains a convolution layer with a kernel size of <i><b>7x7</b></i> and a stride of <i><b>2</b></i>; followed by a max-pool layer of window size <i><b>2x2</b></i> and stride of <i><b>2</b></i>.
                        Similarly, the final block performs full convolution taking feature maps from 64 to 32, followed by 2D-convolution.
                        Finally, we use full-convolution as our classifier with a kernel size of <b><i>2x2</i></b>.
                        </p>
                        <a href="#" class="btn btn-ghost btn-accent btn-small">More details in paper</a>
                    </div>
                </div>
            </div>
        </div>
        <div class="macbook-wrap wp3"></div>
        <div class="responsive-feature-img"><img src="img/arch.png" alt="responsive devices"></div>
    </section>
    <section class="features section-padding" id="features">
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-5">
                    <div class="feature-list">
                        <h3>Encoder and Decoder Blocks</h3>
                        <p align="justify">
                        In encoder and decoder block shown in the right, input and output feature maps for each layer can be calculated using: <b><i>n = 64x2^i</b></i>, where <b><i>i</b></i> the block index.
                        The first encoder block does not perform strided convolution and every convolution layer is followed by batch-normalization and ReLU as non-linearity.
                        The encoder architecture is the same as that of ResNet-18 [3].
                        </p>
                        </br>
                        <table>
                            <tr>
                                <th>Block #</th>
                                <th>Encoder</th>
                                <th>Decoder</th>
                            </tr>
                            <tr>
                                <td>Initial</td>
                                <td>64x64x64</td>
                                <td>(#Classses)x256x256</td>
                            </tr>
                            <tr>
                                <td>1</td>
                                <td>64x64x64</td>
                                <td>64x64x64</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>128x32x32</td>
                                <td>64x64x64</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>256x16x16</td>
                                <td>128x32x32</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>512x8x8</td>
                                <td>256x16x16</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
        <div class="device-showcase">
            <div class="devices">
                <div class="ipad-wrap wp1"></div>
                <div class="iphone-wrap wp2"></div>
            </div>
        </div>
        <div class="responsive-feature-img"><img src="img/devices.png" alt="responsive devices"></div>
    </section>
    <section class="hero-strip">
    </section>
    <section class="blog text-center">
    <!--section class="hero-strip text-center"-->
        <div class="container-fluid">
            <div class="row">
                <div class="col-md-3">
                    <article class="blog-post">
                        <figure>
                            <div class="blog-img-wrap">
                                <div class="overlay">
                                    <i class="fa fa-search"></i>
                                </div>
                                <img src="img/cs_input.gif" alt="Sedna blog image"/>
                            </div>
                            <figcaption>
                            <h2><a class="blog-category" data-toggle="tooltip" data-placement="top" data-original-title="Cityscapes dataset">Cityscapes: Input</a></h2>
                            </figcaption>
                        </figure>
                    </article>
                </div>
                <div class="col-md-3">
                    <article class="blog-post">
                        <figure>
                            <div class="blog-img-wrap">
                                <div class="overlay">
                                    <i class="fa fa-search"></i>
                                </div>
                                <img src="img/cs_pred.gif" alt="Sedna blog image"/>
                            </div>
                            <figcaption>
                            <h2><a class="blog-category" data-toggle="tooltip" data-placement="top" data-original-title="LinkNet performance">Prediction</a></h2>
                            </figcaption>
                        </figure>
                    </article>
                </div>
                <div class="col-md-3">
                    <article class="blog-post">
                        <div class="blog-img-wrap">
                            <div class="overlay">
                                <i class="fa fa-search"></i>
                            </div>
                            <img src="img/cv_input.gif" alt="Sedna blog image"/>
                        </div>
                        <figcaption>
                        <h2><a class="blog-category" data-toggle="tooltip" data-placement="top" data-original-title="CamVid dataset">CamVid: Input</a></h2>
                        </figcaption>
                        </figure>
                    </article>
                </div>
                <div class="col-md-3">
                    <article class="blog-post">
                        <figure>
                            <div class="blog-img-wrap">
                                <div class="overlay">
                                    <i class="fa fa-search"></i>
                                </div>
                                <img src="img/cv_pred.gif" alt="Sedna blog image"/>
                            </div>
                            <figcaption>
                            <h2><a class="blog-category" data-toggle="tooltip" data-placement="top" data-original-title="LinkNet performance">Prediction</a></h2>
                            </figcaption>
                        </figure>
                    </article>
                </div>
            </div>
        </div>
    </section>
    <section class="testimonial-slider section-padding text-center" id="contact">
        <div class="container">
            <div class="row">
                <div class="col-md-6">
                                <div class="avatar"><img src="img/abhi.jpg" alt="Sedna Testimonial Avatar"></div>
                                <h2>Abhishek Chaurasia</h2>
                                <p class="author">aabhish (at) purdue (dot) edu</p>
                </div>
                <div class="col-md-6">
                                <div class="avatar"><img src="img/euge.png" alt="Sedna Testimonial Avatar"></div>
                                <h2>Eugenio Culurciello</h2>
                                <p class="author">euge (at) purdue (dot) edu</p>
                </div>
            </div>
        </div>
    </section>
    <section class="sign-up section-padding text-center" id="download">
        <div class="container">
            <div class="row" align="left">
                <h3>Acknowledgement</h3>
                <p>
                This work was partly supported by the Office of Naval Research (ONR) grants N00014-12-1-0167, N00014-15-1-2791 and MURI N00014-10-1-0278.
                We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TX1, Titan X GPUs used for this research.
                </p>
                <h3>References</h3>
                <p>[1] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
                <p>[2] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, “Segmentation and recognition using structure from motion point clouds,” in ECCV (1), 2008, pp. 44–57.</p>
                <p>[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv preprint arXiv:1512.03385, 2015.</p>
            </div>
        </div>
    </section>
    <section class="to-top">
        <div class="container">
            <div class="row">
                <div class="to-top-wrap">
                    <a href="#top" class="top"><i class="fa fa-angle-up"></i></a>
                </div>
            </div>
            <br>
            <p><font color="white">&copy Abhishek Chaurasia 2017. All rights reserved.</font></p>
        </div>
    </section>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery-1.11.2.min.js"><\/script>')</script>
    <script src="bower_components/retina.js/dist/retina.js"></script>
    <script src="js/jquery.fancybox.pack.js"></script>
    <script src="js/vendor/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
    <script src="js/jquery.flexslider-min.js"></script>
    <script src="bower_components/classie/classie.js"></script>
    <script src="bower_components/jquery-waypoints/lib/jquery.waypoints.min.js"></script>
    <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
    <script>
    (function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=
    function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;
    e=o.createElement(i);r=o.getElementsByTagName(i)[0];
    e.src='//www.google-analytics.com/analytics.js';
    r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
    ga('create','UA-XXXXX-X','auto');ga('send','pageview');
    </script>
</body>
</html>
